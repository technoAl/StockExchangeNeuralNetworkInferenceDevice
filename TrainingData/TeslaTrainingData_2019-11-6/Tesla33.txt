
          Over 40 million developers use GitHub together
          to host and review code, project manage, and build software together
          across more than 100 million projects.
        
              Use Git or checkout with SVN using the web URL.
            
              Want to be notified of new releases in
              microsoft/DialoGPT?
            If nothing happens, download GitHub Desktop and try again.Go backIf nothing happens, download GitHub Desktop and try again.Go backIf nothing happens, download Xcode and try again.Go backIf nothing happens, download the GitHub extension for Visual Studio and try again.Go backThis repository contains the source code and trained model for a large-scale pretrained dialogue response generation model. The human evaluation results indicate that the response generated from DialoGPT is comparable to human response quality under a single-turn conversation Turing test.The repository is based on huggingface pytorch-transformer and OpenAI GPT-2, containing data extraction script, model training code and pretrained small (117M) medium (345M) and large (762M) model checkpoint.The model is trained on 147M multi-turn dialogue from Reddit discussion thread. The largest model can be trained in several hours on a 8 V100 machines (however this is not required), with distributed training and FP16 option.The include script can be used to reproduce the results of DSTC-7 grounded dialogue generation challenge and a 6k multi-reference dataset created from Reddit data.Project webpage: https://www.microsoft.com/en-us/research/project/large-scale-pretraining-for-response-generation/ArXiv paper: https://arxiv.org/abs/1911.00536This code can be run on CPU, but it would be slow. We would recommend to use GPU to train and finetune all models. There is no minimal limit of the number of GPUs. However, if using distributed train for multiple GPUs configuration, the speed-up vs the number of GPUs is roughly sub-linear. To simulate the same batchsize when using less GPUs, please use a larger gradient_accumulation_steps in model training.The 117M and 345M model can be loaded in a single GPU with 12G memory. The 762M model would require a single GPU that has greater than 16G memory for efficient training. The training speed on a benchmark data with 50M training instances and V100 GPUs:Fine-tuning from our pretrained model on a new dataset typically requires 1-2 epochs.We created a demo script demo.py to ease the difficulty of the deployment of this system. The demo.py contains a pipeline of model downloading, data extraction, data preprocessing and model training over a dummy dataset within one commandline.Please use the below commandlines to clone, install the requirements and load the Conda environment (Note that Cuda 10 is required):If you run this on an architecture other than Linux, please use LSP-generic.yml instead of LSP-linux.yml but please note that the generic one is not tested in all platform, so the stablity can not be gauranteed.
To use fp16 training, please install apex by using commands belowTo start, first install the docker and Nvidia-docker from their official repos.
The image environment for running the code can be loaded as below:Nvidia-docker v2.*Nvidia-docker v1.*Inside the docker container, runThis section explains all components in the demo.py.Before running demo.py, you can set DATA_FOLDER (default value ./models)  in demo.py as the place you want to download all the data and pretrained/fine-tuned models. Then simply runtoNote that by default the demo.py will use a dummy data, please specify the Reddit training data by using option --data. Three options are  available:dummy,small and full.The small Reddit data is around 140MB and the full Reddit data is more than 30GB. You can prepare a cup of coffee when processing with the full Reddit data because it takes a long time!The pretrained and fine-tuned models are available on azure blobstorage.
Please run/see demo.py for more details about how to download/use those models. Or you could download directly by using the links in demo_utils.py.First, use the prepare4db.sh to convert a tsv data file into the correct format that the following script can recognize.
The trainig data need to be then processed into a database file with below commandline:The training script can be used in single GPU or multiple GPU settings (distributed training across multiple GPUs within a single node):The training script accept several arguments to tweak the training:During the training, two log files will be updated. The train_log.txt and eval_log.txt contains the model loss, perplexity and training speed (tokens/sec) statistics for the training and dev set.The log file and saved model checkpoint can be found in ./models/output_modelWe note that even with properly filtered Reddit dataset, sometimes our model can still generate moderately toxic/inappropriate responses. Due to this reason, we are unable to provide the decoding script at this time (The live demo and decoding script access is upon invitation only now ).
We are currently still working on a controlled decoding method to prevent this system from toxic generation. Please stay tuned.We release 6 fine-tuned models which can be further fine-tuned on low-resource  user-customized dataset. The total parameters in these models range from 117M to 762M, in accord with OpenAI GPT-2 model sizes.The model files can be loaded exactly as the GPT-2 model checkpoint from Huggingface pytorch-transformer. Please download the required model configuration files (merges.txt, config,json, vocab.json) from ./configs/*.Our model achieved the state-of-the-art results in DSTC-7 Challenge response generation task.where ENT represents the Entropy score, and DIST represents the Distinct score. For all metrics except the average length, larger are better.Note that the superior automatic evaluation comparing to human responses does not necessary imply that our model achieves human parity. Please check out our paper for more detailed analysis.To fine-tune the 345M DialoGPT model on the DSTC-7 challenge data on a server with 8 V100 GPUs, please run the following commandline (The DSTC data can be found at DSTC-7 repo):The trained model can be found at DSTC medium modelPlease downloads the following 3rd-party packages and save into the empty folder 3rdparty:Please follow the DSTC-7 official repo to extract the data, and put data-official-test/test.refs.txt into ./dstc/data/ folder.Run the extraction script below to produce the human response hypothesis file human.resp.txt:Finally, to reproduce the results of human hypothesis on DSTC dataset, please run following commands under the repo folder:The evaluation results will be generated in the folder ./dstc/eval/We test on 6K multi-ref dataset from Reddit (this test data will be release soon). The results are summarized in belowWe further conduct human evaluations (6K examples for each methods, each example is evaluated by 3 human judges). The results show a strong evidence that our generation quality is towards approaching the quality of real human responses, under this non-interactive Turing test:Relevance: A and B, which one is more relevant to the source prompt.Informativeness: A and B, which one is more contentful and informative.Human-Like: A and B, which one do you think is more likely to be generated by Human.Please see full details in our arxiv paper.Please start a issue if you spot any :)The live demo and decoding script access is upon invitation only now. Please stayed tuned for the full release.Microsoft ICECAPS: https://github.com/microsoft/icecaps.As an orthogonal repository of this project,
Microsoft Icecaps is an open-source toolkit (in tensorflow) for building neural conversational systems. Icecaps provides an array of tools from recent conversation modeling and general NLP literature within a flexible paradigm that enables complex multi-task learning setups.Pretrained UniLM: https://github.com/microsoft/unilmMT-DNN: https://github.com/namisan/mt-dnnPlease contact DialoGPT@microsoft.com if you have any questions/suggestions. However, the response will be sporadic. Please expect delay.This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.This project has adopted the Microsoft Open Source Code of Conduct.
For more information see the Code of Conduct FAQ or
contact opencode@microsoft.com with any additional questions or comments.This repository aims to facilitate research in large-scale pretraining for conversational data. This toolkit contains only part of the modeling machinery needed to actually produce a model weight file in a running dialog. On its own, this model provides only information about the weights of various text spans; in order for a researcher to actually use it, they will need to bring conversational data of their own and decode the response generation from the pretrained system. Microsoft is not responsible for any generation from the 3rd party utilization of the pretrained system.If you use this code in your research, you can cite our arxiv paper: