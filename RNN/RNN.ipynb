{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "dataset, info = tfds.load('imdb_reviews/subwords8k', with_info=True,\n",
    "                          as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 8185\n"
     ]
    }
   ],
   "source": [
    "encoder = info.features['text'].encoder\n",
    "print ('Vocabulary size: {}'.format(encoder.vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "for i in range(14,32):\n",
    "    try:\n",
    "        for j in range(100):\n",
    "            with open(\"../TrainingData/TeslaTrainingData_2019-10-\"+str(i)+\"/Tesla\"+str(j)+\".txt\", mode='rb') as file:\n",
    "                try:\n",
    "                    a.append(str(file.read()))\n",
    "                except:\n",
    "                    continue\n",
    "    except:\n",
    "        continue\n",
    "for i in range(1,14):\n",
    "    try:\n",
    "        for j in range(100):\n",
    "            with open(\"../TrainingData/TeslaTrainingData_2019-11-\"+str(i)+\"/Tesla\"+str(j)+\".txt\", mode='rb') as file:\n",
    "                try:\n",
    "                    a.append(str(file.read()))\n",
    "                except:\n",
    "                    continue\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2300,)\n",
      "(2300,)\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 300)          15000000  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 100, 256)          440320    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 15,458,881\n",
      "Trainable params: 15,458,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model Training occurs here\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "from tensorflow.keras import layers\n",
    "x_train = np.array(a, dtype=np.str)\n",
    "x_train = x_train.astype(str)\n",
    "print(x_train.shape)\n",
    "y_train = np.zeros(2300) + 100\n",
    "print(y_train.shape)\n",
    "import tensorflow as tf\n",
    "embed_size = 300     # how big is each word vector\n",
    "max_features = 50000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 100\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(x_train))\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "model1 = tf.keras.Sequential()\n",
    "model1.add(Embedding(max_features, embed_size, input_length=maxlen))\n",
    "model1.add(Bidirectional(CuDNNLSTM(128, return_sequences=True)))\n",
    "model1.add(GlobalMaxPool1D())\n",
    "model1.add(Dropout(0.2))\n",
    "model1.add(Dense(64, activation='relu'))\n",
    "model1.add(Dropout(0.2))\n",
    "model1.add(Dense(32, activation='relu'))\n",
    "model1.add(Dropout(0.2))\n",
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model1.summary()\n",
    "model1.compile(loss='binary_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    2     3  5846 18313     1  5846     2   572    88   615  3172   903\n",
      "     4    58   672     9    41     6  9234 12519    42    22    91  3388\n",
      "    11   499     2   174  5837     7    66    19     3   180   792   697\n",
      "     1   111   917   220   366    37   312   404    11    40 18314     8\n",
      "     1 18315   599    47  1528    13     2  1732  5611    26   636     2\n",
      "   636   235   491  1678     7    66    19     1   170  5612    11   155\n",
      "    17     3   790  1580  2964 12529  7402    37  1066    11  9619 18316\n",
      "     1   910  6625     4 18317    58   776  1346     1 10036  6292  1701\n",
      " 10037    26  1850   112]\n",
      "Train on 2300 samples\n",
      "Epoch 1/100\n",
      "2300/2300 [==============================] - ETA: 13s - loss: -239.3672 - accuracy: 0.0000e+ - ETA: 13s - loss: -251.6477 - accuracy: 0.0000e+ - ETA: 13s - loss: -262.7136 - accuracy: 0.0000e+ - ETA: 13s - loss: -273.2399 - accuracy: 0.0000e+ - ETA: 13s - loss: -283.1161 - accuracy: 0.0000e+ - ETA: 13s - loss: -295.8396 - accuracy: 0.0000e+ - ETA: 12s - loss: -310.8861 - accuracy: 0.0000e+ - ETA: 12s - loss: -322.3570 - accuracy: 0.0000e+ - ETA: 12s - loss: -328.8437 - accuracy: 0.0000e+ - ETA: 12s - loss: -340.4296 - accuracy: 0.0000e+ - ETA: 12s - loss: -353.4642 - accuracy: 0.0000e+ - ETA: 11s - loss: -366.5242 - accuracy: 0.0000e+ - ETA: 11s - loss: -378.1419 - accuracy: 0.0000e+ - ETA: 11s - loss: -386.8396 - accuracy: 0.0000e+ - ETA: 11s - loss: -396.7757 - accuracy: 0.0000e+ - ETA: 11s - loss: -404.8312 - accuracy: 0.0000e+ - ETA: 10s - loss: -414.2544 - accuracy: 0.0000e+ - ETA: 10s - loss: -422.1512 - accuracy: 0.0000e+ - ETA: 10s - loss: -431.6896 - accuracy: 0.0000e+ - ETA: 10s - loss: -442.5757 - accuracy: 0.0000e+ - ETA: 10s - loss: -453.8829 - accuracy: 0.0000e+ - ETA: 9s - loss: -464.6947 - accuracy: 0.0000e+00 - ETA: 9s - loss: -473.7587 - accuracy: 0.0000e+0 - ETA: 9s - loss: -482.7226 - accuracy: 0.0000e+0 - ETA: 9s - loss: -489.5916 - accuracy: 0.0000e+0 - ETA: 9s - loss: -497.2866 - accuracy: 0.0000e+0 - ETA: 8s - loss: -506.2452 - accuracy: 0.0000e+0 - ETA: 8s - loss: -517.4467 - accuracy: 0.0000e+0 - ETA: 8s - loss: -525.6865 - accuracy: 0.0000e+0 - ETA: 8s - loss: -533.1812 - accuracy: 0.0000e+0 - ETA: 8s - loss: -543.2370 - accuracy: 0.0000e+0 - ETA: 7s - loss: -554.2783 - accuracy: 0.0000e+0 - ETA: 7s - loss: -562.3641 - accuracy: 0.0000e+0 - ETA: 7s - loss: -571.4316 - accuracy: 0.0000e+0 - ETA: 7s - loss: -579.3975 - accuracy: 0.0000e+0 - ETA: 7s - loss: -589.1674 - accuracy: 0.0000e+0 - ETA: 6s - loss: -598.6368 - accuracy: 0.0000e+0 - ETA: 6s - loss: -607.2347 - accuracy: 0.0000e+0 - ETA: 6s - loss: -614.1578 - accuracy: 0.0000e+0 - ETA: 6s - loss: -624.0477 - accuracy: 0.0000e+0 - ETA: 6s - loss: -635.3213 - accuracy: 0.0000e+0 - ETA: 5s - loss: -645.7402 - accuracy: 0.0000e+0 - ETA: 5s - loss: -655.9128 - accuracy: 0.0000e+0 - ETA: 5s - loss: -666.0378 - accuracy: 0.0000e+0 - ETA: 5s - loss: -675.0282 - accuracy: 0.0000e+0 - ETA: 5s - loss: -683.0302 - accuracy: 0.0000e+0 - ETA: 4s - loss: -693.7969 - accuracy: 0.0000e+0 - ETA: 4s - loss: -701.9383 - accuracy: 0.0000e+0 - ETA: 4s - loss: -710.1337 - accuracy: 0.0000e+0 - ETA: 4s - loss: -719.3181 - accuracy: 0.0000e+0 - ETA: 4s - loss: -727.1635 - accuracy: 0.0000e+0 - ETA: 3s - loss: -734.5294 - accuracy: 0.0000e+0 - ETA: 3s - loss: -740.9630 - accuracy: 0.0000e+0 - ETA: 3s - loss: -750.7870 - accuracy: 0.0000e+0 - ETA: 3s - loss: -758.2717 - accuracy: 0.0000e+0 - ETA: 3s - loss: -767.6005 - accuracy: 0.0000e+0 - ETA: 2s - loss: -775.9347 - accuracy: 0.0000e+0 - ETA: 2s - loss: -785.0666 - accuracy: 0.0000e+0 - ETA: 2s - loss: -794.5032 - accuracy: 0.0000e+0 - ETA: 2s - loss: -805.3347 - accuracy: 0.0000e+0 - ETA: 2s - loss: -813.1680 - accuracy: 0.0000e+0 - ETA: 1s - loss: -822.2216 - accuracy: 0.0000e+0 - ETA: 1s - loss: -831.8746 - accuracy: 0.0000e+0 - ETA: 1s - loss: -841.0121 - accuracy: 0.0000e+0 - ETA: 1s - loss: -849.7598 - accuracy: 0.0000e+0 - ETA: 1s - loss: -859.8379 - accuracy: 0.0000e+0 - ETA: 0s - loss: -871.6660 - accuracy: 0.0000e+0 - ETA: 0s - loss: -882.2319 - accuracy: 0.0000e+0 - ETA: 0s - loss: -892.9542 - accuracy: 0.0000e+0 - ETA: 0s - loss: -902.2053 - accuracy: 0.0000e+0 - ETA: 0s - loss: -912.0604 - accuracy: 0.0000e+0 - 14s 6ms/sample - loss: -920.7724 - accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      "2300/2300 [==============================] - ETA: 13s - loss: -1657.4363 - accuracy: 0.0000e+0 - ETA: 13s - loss: -1647.7908 - accuracy: 0.0000e+0 - ETA: 13s - loss: -1653.8593 - accuracy: 0.0000e+0 - ETA: 13s - loss: -1673.0503 - accuracy: 0.0000e+0 - ETA: 13s - loss: -1693.8055 - accuracy: 0.0000e+0 - ETA: 13s - loss: -1719.6792 - accuracy: 0.0000e+0 - ETA: 12s - loss: -1716.5239 - accuracy: 0.0000e+0 - ETA: 12s - loss: -1722.1132 - accuracy: 0.0000e+0 - ETA: 12s - loss: -1720.8122 - accuracy: 0.0000e+0 - ETA: 12s - loss: -1735.0018 - accuracy: 0.0000e+0 - ETA: 12s - loss: -1744.1884 - accuracy: 0.0000e+0 - ETA: 11s - loss: -1750.6579 - accuracy: 0.0000e+0 - ETA: 11s - loss: -1740.3717 - accuracy: 0.0000e+0 - ETA: 11s - loss: -1745.3637 - accuracy: 0.0000e+0 - ETA: 11s - loss: -1758.2028 - accuracy: 0.0000e+0 - ETA: 11s - loss: -1767.0765 - accuracy: 0.0000e+0 - ETA: 10s - loss: -1779.6846 - accuracy: 0.0000e+0 - ETA: 10s - loss: -1782.5963 - accuracy: 0.0000e+0 - ETA: 10s - loss: -1794.0314 - accuracy: 0.0000e+0 - ETA: 10s - loss: -1804.5994 - accuracy: 0.0000e+0 - ETA: 10s - loss: -1813.9267 - accuracy: 0.0000e+0 - ETA: 9s - loss: -1827.2083 - accuracy: 0.0000e+0 - ETA: 9s - loss: -1839.8351 - accuracy: 0.0000e+ - ETA: 9s - loss: -1850.4401 - accuracy: 0.0000e+ - ETA: 9s - loss: -1863.1846 - accuracy: 0.0000e+ - ETA: 9s - loss: -1875.3486 - accuracy: 0.0000e+ - ETA: 8s - loss: -1883.1034 - accuracy: 0.0000e+ - ETA: 8s - loss: -1899.8336 - accuracy: 0.0000e+ - ETA: 8s - loss: -1910.4967 - accuracy: 0.0000e+ - ETA: 8s - loss: -1915.1354 - accuracy: 0.0000e+ - ETA: 8s - loss: -1930.6922 - accuracy: 0.0000e+ - ETA: 7s - loss: -1947.7150 - accuracy: 0.0000e+ - ETA: 7s - loss: -1956.2596 - accuracy: 0.0000e+ - ETA: 7s - loss: -1968.1556 - accuracy: 0.0000e+ - ETA: 7s - loss: -1982.8039 - accuracy: 0.0000e+ - ETA: 7s - loss: -1998.7176 - accuracy: 0.0000e+ - ETA: 7s - loss: -2008.9042 - accuracy: 0.0000e+ - ETA: 6s - loss: -2025.7286 - accuracy: 0.0000e+ - ETA: 6s - loss: -2040.2720 - accuracy: 0.0000e+ - ETA: 6s - loss: -2047.3640 - accuracy: 0.0000e+ - ETA: 6s - loss: -2062.5994 - accuracy: 0.0000e+ - ETA: 6s - loss: -2072.3275 - accuracy: 0.0000e+ - ETA: 5s - loss: -2084.6257 - accuracy: 0.0000e+ - ETA: 5s - loss: -2099.2427 - accuracy: 0.0000e+ - ETA: 5s - loss: -2111.0787 - accuracy: 0.0000e+ - ETA: 5s - loss: -2124.2014 - accuracy: 0.0000e+ - ETA: 4s - loss: -2137.9758 - accuracy: 0.0000e+ - ETA: 4s - loss: -2147.8506 - accuracy: 0.0000e+ - ETA: 4s - loss: -2158.5529 - accuracy: 0.0000e+ - ETA: 4s - loss: -2173.5837 - accuracy: 0.0000e+ - ETA: 4s - loss: -2182.3506 - accuracy: 0.0000e+ - ETA: 3s - loss: -2193.7183 - accuracy: 0.0000e+ - ETA: 3s - loss: -2205.4929 - accuracy: 0.0000e+ - ETA: 3s - loss: -2217.3293 - accuracy: 0.0000e+ - ETA: 3s - loss: -2230.2561 - accuracy: 0.0000e+ - ETA: 3s - loss: -2240.9295 - accuracy: 0.0000e+ - ETA: 2s - loss: -2252.1444 - accuracy: 0.0000e+ - ETA: 2s - loss: -2264.9802 - accuracy: 0.0000e+ - ETA: 2s - loss: -2277.4180 - accuracy: 0.0000e+ - ETA: 2s - loss: -2286.9338 - accuracy: 0.0000e+ - ETA: 2s - loss: -2297.9224 - accuracy: 0.0000e+ - ETA: 1s - loss: -2304.9444 - accuracy: 0.0000e+ - ETA: 1s - loss: -2318.8699 - accuracy: 0.0000e+ - ETA: 1s - loss: -2334.8428 - accuracy: 0.0000e+ - ETA: 1s - loss: -2346.9640 - accuracy: 0.0000e+ - ETA: 1s - loss: -2359.0217 - accuracy: 0.0000e+ - ETA: 0s - loss: -2372.2867 - accuracy: 0.0000e+ - ETA: 0s - loss: -2385.4899 - accuracy: 0.0000e+ - ETA: 0s - loss: -2397.6311 - accuracy: 0.0000e+ - ETA: 0s - loss: -2411.2699 - accuracy: 0.0000e+ - ETA: 0s - loss: -2428.1354 - accuracy: 0.0000e+ - 14s 6ms/sample - loss: -2438.6475 - accuracy: 0.0000e+00\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 288/2300 [==>...........................] - ETA: 13s - loss: -3409.2551 - accuracy: 0.0000e+0 - ETA: 13s - loss: -3419.2020 - accuracy: 0.0000e+0 - ETA: 13s - loss: -3377.7164 - accuracy: 0.0000e+0 - ETA: 13s - loss: -3421.7670 - accuracy: 0.0000e+0 - ETA: 13s - loss: -3435.4788 - accuracy: 0.0000e+0 - ETA: 13s - loss: -3450.2326 - accuracy: 0.0000e+0 - ETA: 12s - loss: -3481.4294 - accuracy: 0.0000e+0 - ETA: 12s - loss: -3496.8812 - accuracy: 0.0000e+0 - ETA: 12s - loss: -3520.7014 - accuracy: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "print(x_train[0])\n",
    "model1.fit(x_train, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(9):\n",
    "    encoder.encode(x_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_size(vec, size):\n",
    "    zeros = [0] * (size - len(vec))\n",
    "    vec.extend(zeros)\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_predict(sentence, pad):\n",
    "    encoded_sample_pred_text = encoder.encode(sample_pred_text)\n",
    "\n",
    "    if pad:\n",
    "        encoded_sample_pred_text = pad_to_size(encoded_sample_pred_text, 64)\n",
    "    encoded_sample_pred_text = tf.cast(encoded_sample_pred_text, tf.float32)\n",
    "    predictions = model.predict(tf.expand_dims(encoded_sample_pred_text, 0))\n",
    "\n",
    "    return (predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on a sample text without padding.\n",
    "\n",
    "sample_pred_text = ('Tesla is doing really well now. It is going to be one of the greatest companies ever in the near future')\n",
    "predictions = sample_predict(sample_pred_text, pad=False)\n",
    "print (predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z','A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z,',1,2,3,4,5,6,7,8,9,0,'.',',','!','?','#','%']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
